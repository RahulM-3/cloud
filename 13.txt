from pyspark.sql import SparkSession
spark SparkSession.builder.appName("WordCount").getOrCreate()
file_path = "sample.txt"
text_rdd = spark.sparkContext.textFile(file_path)
word counts = (
text_rdd
flatMap(lambda line: line.split()) #Tokenize each line into words
.map(lambda word: (word, 1)) #Map each word to (word, 1)
reduceByKey(lambda a, b: a+b) #Sum counts per word
)
print("\nWord Count Results:")
for word, count in word_counts.collect():
print(f"(word): (count)")
spark.stop()
